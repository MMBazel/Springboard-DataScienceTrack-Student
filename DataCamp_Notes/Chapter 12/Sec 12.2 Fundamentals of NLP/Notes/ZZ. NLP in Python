Natural Language Processing Fundamentals in Python

Course Description
In this course, you'll learn Natural Language Processing (NLP) basics, such as how to identify and separate words, how to extract topics in a text, and how to build your own fake news classifier. You'll also learn how to use basic libraries such as NLTK, alongside libraries which utilize deep learning to solve common NLP problems. This course will give you the foundation to process and parse text as you move forward in your Python learning.

===============================================================================================================================

1
Regular expressions & word tokenization
FREE
0%
This chapter will introduce some basic NLP concepts, such as word tokenization and regular expressions to help parse text. You'll also learn how to handle non-English text and more difficult tokenization you might find as you explore the wide world of NLP.

_____________________________________________________________________________________________________________________________

Introduction to regular
expressions

What is Natural Language Processing?
Field of study focused on making sense of language
Using statistics and computers
You will learn the basics of NLP
Topic identification
Text classification
NLP applications include:
Chatbots
Translation
Sentiment analysis
... and many more!



What exactly are regular expressions?
Strings with a special syntax
Allow us to match patterns in other strings
Applications of regular expressions:
Find all web links in a document
Parse email addresses, remove/replace unwanted characters

In [1]: import re
In [2]: re.match('abc'
,
'abcdef')
Out[2]: <_sre.SRE_Match object; span=(0, 3), match='abc'>
In [3]: word_regex = '\w+'
In [4]: re.match(word_regex,
'hi there!')
Out[4]: <_sre.SRE_Match object; span=(0, 2), match='hi'>


Common regex patterns (7)
pattern matches example
\w+ word 'Magic'
\d digit 9
\s space ' '
.* wildcard 'username74'
+ or * greedy match 'aaaaaa'
\S not space 'no_spaces'
[a-z] lowercase group 'abcdefg




Python's re Module
re module
split: split a string on regex
findall: find all patterns in a string
search: search for a pattern
match: match an entire string or substring based on a pattern
Pattern first, and the string second
May return an iterator, string, or match object
In [5]: re.split('\s+'
,
'Split on spaces.')
Out[5]: ['Split'
,
'on'
,
'spaces.']

________________________________________________________________________________________________________________________

Practicing regular expressions: re.split() and re.findall()
Now you'll get a chance to write some regular expressions to match digits, strings and non-alphanumeric characters. Take a look at my_string first by printing it in the IPython Shell, to determine how you might best match the different steps.

Note: It's important to prefix your regex patterns with r to ensure that your patterns are interpreted in the way you want them to. Else, you may encounter problems to do with escape sequences in strings. For example, "\n" in Python is used to indicate a new line, but if you use the r prefix, it will be interpreted as the raw string "\n" - that is, the character "\" followed by the character "n" - and not as a new line.

Instructions
100 XP
Instructions
100 XP
Import the regular expression module re.
Split my_string on each sentence ending. To do this:
Write a pattern called sentence_endings to match sentence endings (., ?, and !).
Use re.split() to split my_string on the pattern and print the result.
Find and print all capitalized words in my_string by writing a pattern called capitalized_words and using re.findall().
Remember the [a-z] pattern shown in the video to match lowercase groups? Modify that pattern appropriately in order to match uppercase groups.
Write a pattern called spaces to match one or more spaces ("\s+") and then use re.split() to split my_string on this pattern, keeping all punctuation intact. Print the result.
Find all digits in my_string by writing a pattern called digits ("\d+") and using re.findall(). Print the result.


# Import the regex module
import re

# Write a pattern to match sentence endings: sentence_endings
sentence_endings = r"[.?!]"

# Split my_string on sentence endings and print the result
print(re.split(sentence_endings, my_string))

# Find all capitalized words in my_string and print the result
capitalized_words = r"[A-Z]\w+"
print(re.findall(capitalized_words, my_string))

# Split my_string on spaces and print the result
spaces = r"\s+"
print(re.split(spaces, my_string))

# Find all digits in my_string and print the result
digits = r"\d+"
print(re.findall(digits, my_string))




________________________________________________________________________________________________________________________


Introduction to
tokenization


What is tokenization?
Turning a string or document into tokens (smaller chunks)
One step in preparing a text for NLP
Many different theories and rules
You can create your own rules using regular expressions
Some examples:
Breaking out words or sentences
Separating punctuation
Separating all hashtags in a tweet


nltk library
nltk: natural language toolkit
In [1]: from nltk.tokenize import word_tokenize
In [2]: word_tokenize("Hi there!")
Out[2]: ['Hi'
,
'there'
,
'!']


Why tokenize?
Easier to map part of speech
Matching common words
Removing unwanted tokens
"I don't like Sam's shoes."
"I"
,
"do"
,
"n't"
,
"like"
,
"Sam"
,
"'s"
,
"shoes"
,
"."


Other nltk tokenizers
sent_tokenize: tokenize a document into sentences
regexp_tokenize: tokenize a string or document based on a regular
expression pattern
TweetTokenizer: special class just for tweet tokenization, allowing you
to separate hashtags, mentions and lots of exclamation points!!!



More regex practice
Difference between re.search() and re.match()
In [1]: import re
In [2]: re.match('abc'
,
'abcde')
Out[2]: <_sre.SRE_Match object; span=(0, 3), match='abc'>
In [3]: re.search('abc'
,
'abcde')
Out[3]: <_sre.SRE_Match object; span=(0, 3), match='abc'>
In [4]: re.match('cd'
,
'abcde')
In [5]: re.search('cd'
,
'abcde')
Out[5]: <_sre.SRE_Match object; span=(2, 4), match='cd'>

_____________________________________________________________________________________________________________________


Word tokenization with NLTK
Here, you'll be using the first scene of Monty Python's Holy Grail, which has been pre-loaded as scene_one. Feel free to check it out in the IPython Shell!

Your job in this exercise is to utilize word_tokenize and sent_tokenize from nltk.tokenize to tokenize both words and sentences from Python strings - in this case, the first scene of Monty Python's Holy Grail.

Instructions
100 XP
Import the sent_tokenize and word_tokenize functions from nltk.tokenize.
Tokenize all the sentences in scene_one using the sent_tokenize() function.
Tokenize the fourth sentence in sentences, which you can access as sentences[3], using the word_tokenize() function.
Find the unique tokens in the entire scene by using word_tokenize() on scene_one and then converting it into a set using set().
Print the unique tokens found. This has been done for you, so hit 'Submit Answer' to see the results!


# Import necessary modules
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize

# Split scene_one into sentences: sentences
sentences = sent_tokenize(scene_one)

# Use word_tokenize to tokenize the fourth sentence: tokenized_sent
tokenized_sent = word_tokenize(sentences[3])

# Make a set of unique tokens in the entire scene: unique_tokens
unique_tokens = set(word_tokenize(scene_one))

# Print the unique tokens result
print(unique_tokens)


<script.py> output:
    {'master', 'seek', 'wants', 'suggesting', 'European', 'climes', 'an', '[', 'Not', 'Please', 'ounce', 'all', 'Court', 'anyway', "'ve", 'matter', 'feathers', 'may', 'here', 'castle', 'weight', 'Mercea', 'forty-three', 'they', ',', 'son', 'using', 'martin', 'times', 'point', 'your', 'its', 'minute', 'two', 'by', 'simple', 'lord', 'ratios', 'It', 'does', 'question', 'through', 'non-migratory', 'but', 'defeator', 'warmer', 'goes', 'Whoa', 'guiding', 'beat', 'you', '...', 'Oh', 'We', 'So', '--', 'not', 'needs', 'must', '?', 'a', 'They', 'why', 'use', 'halves', 'Listen', 'will', 'maintain', 'migrate', 'then', ']', 'temperate', 'line', 'and', 'That', 'swallow', 'breadth', 'one', 'Well', 'agree', 'under', 'pound', 'snows', 'them', "'s", 'do', 'England', 'go', 'wings', 'second', 'our', 'got', 'Yes', 'since', 'African', 'five', 'my', 'grip', 'course', 'yeah', 'Saxons', 'me', 'get', 'strand', 'at', ':', 'air-speed', 'bangin', 'mean', 'he', 'of', 'order', 'KING', 'zone', 'Pull', 'with', 'Supposing', 'creeper', "n't", 'A', 'Will', 'if', 'ask', 'search', 'am', '!', 'other', 'carried', 'coconuts', '#', 'Uther', 'strangers', 'be', 'coconut', 'I', 'dorsal', 'just', 'back', 'held', 'empty', 'SOLDIER', 'bird', 'plover', 'who', 'tell', 'Arthur', 'court', 'Camelot', 'right', 'sovereign', 'wind', 'Who', 'yet', 'south', '2', 'together', 'could', 'is', 'Patsy', 'there', 'or', 'bring', 'Wait', 'No', 'are', '.', 'The', 'it', 'length', 'the', 'Are', 'kingdom', 'SCENE', 'carrying', 'land', 'velocity', 'But', 'You', 'this', 'winter', 'from', "'em", 'to', 'Am', '1', "'m", 'house', 'swallows', 'grips', 'husk', 'servant', "'d", 'sun', 'covered', 'Halt', 'join', 'Pendragon', 'Britons', 'tropical', 'fly', 'King', 'have', 'Where', 'maybe', "'", 'ARTHUR', 'Found', 'every', 'found', 'in', 'where', 'knights', 'ridden', 'interested', 'Ridden', 'carry', 'In', 'these', 'on', 'horse', 'trusty', 'that', 'What', 'clop', 'speak', "'re"}

________________________________________________________________________________________________________________________

More regex with re.search()
In this exercise, you'll utilize re.search() and re.match() to find specific tokens. Both search and match expect regex patterns, similar to those you defined in an earlier exercise. You'll apply these regex library methods to the same Monty Python text from the nltk corpora.

You have both scene_one and sentences available from the last exercise; now you can use them with re.search() and re.match() to extract and match more text.

Instructions
100 XP
Use re.search() to search for the first occurance of the word "coconuts" in scene_one. Store the result in match.
Print the start and end indexes of match using its .start() and .end() methods, respectively.
Write a regular expression called pattern1 to find anything in square brackets.
Use re.search() with the previous pattern to find the first text in square brackets in the scene. Print the result.
Use re.match() to match the script notation in the fourth line (ARTHUR:) and print the result. The tokenized sentences of scene_one are available in your namespace as sentences.


# Search for the first occurrence of "coconuts" in scene_one: match
match = re.search("coconuts", scene_one)

# Print the start and end indexes of match
print(match.start(), match.end())

# Write a regular expression to search for anything in square brackets: pattern1
pattern1 = r"\[.*\]"

# Use re.search to find the first text in square brackets
print(re.search(pattern1, scene_one))

# Find the script notation at the beginning of the fourth sentence and print it
pattern2 = r"[\w\s]+:"
print(re.match(pattern2, sentences[3]))



________________________________________________________________________________________________________________________

Advanced tokenization
with regex

Regex groups using or "|"
OR is represented using |
You can define a group using ()
You can define explicit character ranges using []

In [1]: import re
In [2]: match_digits_and_words = ('(\d+|\w+)')
In [3]: re.findall(match_digits_and_words,
'He has 11 cats.')
Out[3]: ['He'
,
'has'
,
'11'
,
'cats']



Regex ranges and groups
pattern matches example
[A-Za-z]+ upper and lowercase English alphabet 'ABCDEFghijk'
[0-9] numbers from 0 to 9 9
[A-Za-z\-
\.]+
upper and lowercase English alphabet, -
and .
'MyWebsite.com'
(a-z) a, - and z 'a-z'
(\s+l,) spaces or a comma '
,
'



Character range with re.match()
In [1]: import re
In [2]: my_str = 'match lowercase spaces nums like 12, but no commas'
In [3]: re.match('[a-z0-9 ]+'
, my_str)
Out[3]: <_sre.SRE_Match object;
span=(0, 42), match='match lowercase spaces nums like 12'>

________________________________________________________________________________________________________________________

Regex with NLTK tokenization
Twitter is a frequently used source for NLP text and tasks. In this exercise, you'll build a more complex tokenizer for tweets with hashtags and mentions using nltk and regex. The nltk.tokenize.TweetTokenizer class gives you some extra methods and attributes for parsing tweets.

Here, you're given some example tweets to parse using both TweetTokenizer and regexp_tokenize from the nltk.tokenize module. These example tweets have been pre-loaded into the variable tweets. Feel free to explore it in the IPython Shell!

Instructions
100 XP
Instructions
100 XP
From nltk.tokenize, import regexp_tokenize and TweetTokenizer.
A regex pattern to define hashtags called pattern1 has been defined for you. Call regexp_tokenize() with this hashtag pattern on the first tweet in tweets.
Write a new pattern called pattern2 to match mentions and hashtags. A mention is something like @DataCamp. Then, call regexp_tokenize() with your new hashtag pattern on the last tweet in tweets. You can access the last element of a list using -1 as the index, for example, tweets[-1].
Create an instance of TweetTokenizer called tknzr and use it inside a list comprehension to tokenize each tweet into a new list called all_tokens. To do this, use the .tokenize() method of tknzr, with t as your iterator variable.


# Import the necessary modules
from nltk.tokenize import regexp_tokenize
from nltk.tokenize import TweetTokenizer

# Define a regex pattern to find hashtags: pattern1
pattern1 = r"#\w+"

# Use the pattern on the first tweet in the tweets list
regexp_tokenize(tweets[0], pattern1)

# Write a pattern that matches both mentions and hashtags
pattern2 = r"([@]\w+)"

# Use the pattern on the last tweet in the tweets list
regexp_tokenize(tweets[-1], pattern2)

# Use the TweetTokenizer to tokenize all tweets into one list
tknzr = TweetTokenizer()
all_tokens = [tknzr.tokenize(t) for t in tweets]
print(all_tokens)


<script.py> output:
    [['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]
    
________________________________________________________________________________________________________________________


Non-ascii tokenization
In this exercise, you'll practice advanced tokenization by tokenizing some non-ascii based text. You'll be using German with emoji!

Here, you have access to a string called german_text, which has been printed for you in the Shell. Notice the emoji and the German characters!

The following modules have been pre-imported from nltk.tokenize: regexp_tokenize and word_tokenize.

Unicode ranges for emoji are:

('\U0001F300'-'\U0001F5FF'), ('\U0001F600-\U0001F64F'), ('\U0001F680-\U0001F6FF'), and ('\u2600'-\u26FF-\u2700-\u27BF').

Instructions
100 XP
Tokenize all the words in german_text using word_tokenize(), and print the result.
Tokenize only the capital words in german_text.
First, write a pattern called capital_words to match only capital words. Make sure to check for the German √ú!
Then, tokenize it using regexp_tokenize().
Tokenize only the emoji in german_text. The pattern using the unicode ranges for emoji given in the assignment text has been written for you. Your job is to use regexp_tokenize() to tokenize the emoji.

# Tokenize and print all words in german_text
all_words = word_tokenize(german_text)
print(all_words)

# Tokenize and print only capital words
capital_words = r"[A-Z√ú]\w+"
print(regexp_tokenize(german_text, capital_words))

# Tokenize and print only emoji
emoji = "['\U0001F300-\U0001F5FF'|'\U0001F600-\U0001F64F'|'\U0001F680-\U0001F6FF'|'\u2600-\u26FF\u2700-\u27BF']"
print(regexp_tokenize(german_text, emoji))

<script.py> output:
    ['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', 'üçï', 'Und', 'f√§hrst', 'du', 'mit', '√úber', '?', 'üöï']
    ['Wann', 'Pizza', 'Und', '√úber']
    ['üçï', 'üöï']




________________________________________________________________________________________________________________________

Charting word length
with nltk


Getting started with matplotlib
Charting library used by many open source Python projects
Straightforward functionality with lots of options
Histograms
Bar charts
Line charts
Scatter plots
... and also advanced functionality like 3D graphs and animations!


Plotting a histogram with matplotlib
In [1]: from matplotlib import pyplot as plt
In [2]: plt.hist([1, 5, 5, 7, 7, 7, 9])
Out[2]: (array([ 1., 0., 0., 0., 0., 2., 0., 3., 0., 1.]),
array([ 1. , 1.8, 2.6, 3.4, 4.2, 5. , 5.8, 6.6,
7.4, 8.2, 9. ]),
<a list of 10 Patch objects>)
In [3]: plt.show()


Generated Histogram


Combining NLP data extraction with plotting
In [1]: from matplotlib import pyplot as plt
In [2]: from nltk.tokenize import word_tokenize
In [3]: words = word_tokenize("This is a pretty cool tool!")
In [4]: word_lengths = [len(w) for w in words]
In [5]: plt.hist(word_lengths)
Out[5]: (array([ 2., 0., 1., 0., 0., 0., 3., 0., 0., 1.]),
array([ 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. , 5.5,
6. ]),
<a list of 10 Patch objects>)
In [6]: plt.show()









________________________________________________________________________________________________________________________

# Split the script into lines: lines
lines = holy_grail.split('\n')

# Replace all script lines for speaker
pattern = "[A-Z]{2,}(\s)?(#\d)?([A-Z]{2,})?:"
lines = [re.sub(pattern, '', l) for l in lines]

# Tokenize each line: tokenized_lines
tokenized_lines = [regexp_tokenize(s,"\w+") for s in lines]

# Make a frequency list of lengths: line_num_words
line_num_words = [len(t_line) for t_line in tokenized_lines]

# Plot a histogram of the line lengths
plt.hist(line_num_words)

# Show the plot
plt.show()


Charting practice
Try using your new skills to find and chart the number of words per line in the script using matplotlib. The Holy Grail script is loaded for you, and you need to use regex to find the words per line.

Using list comprehensions here will speed up your computations. For example: my_lines = [tokenize(l) for l in lines] will call a function tokenize on each line in the list lines. The new transformed list will be saved in the my_lines variable.

You have access to the entire script in the variable holy_grail. Go for it!

Instructions
70 XP
Split the script into lines using the newline ('\n') character.
Use re.sub() inside a list comprehension to replace the prompts such as ARTHUR: and SOLDIER #1. The pattern has been written for you.
Use a list comprehension to tokenize lines with regexp_tokenize(), keeping only words. Recall that the pattern for words is "\w+".
Use a list comprehension to create a list of line lengths called line_num_words.
Use t_line as your iterator variable to iterate over tokenized_lines, and then len() function to compute line lengths.
Plot a histogram of line_num_words using plt.hist(). Don't forgot to use plt.show() as well to display the plot.


Hint
Use the .split() method on holy_grail with the newline character ('\n') as the argument.
Recall that re.sub() requires 3 arguments: The pattern, the replacement, and the string. The pattern is given for you; the replacement is '' and the string is l.
Use regexp_tokenize() as the output expression of your list comprehension, with s and "\w+" as the arguments.
To create line_num_words, use len(t_line) as the output expression of the list comprehension.
Use plt.hist() with line_num_words as the argument to create the histogram, and then plt.show() to display it.


===============================================================================================================================

2
Simple topic identification
0%
This chapter will introduce you to topic identification, which you can apply to any text you encounter in the wild. Using basic NLP models, you will identify topics from texts based on term frequencies. You'll experiment and compare two simple methods - bag-of-words and Tf-idf using NLTK and a new library - Gensim.

Word counts with bagof-words
Bag-of-words
Basic method for finding topics in a text
Need to first create tokens using tokenization
... and then count up all the tokens
The more frequent a word, the more important it might be
Can be a great way to determine the significant words in a text


Bag-of-words example
Text: "The cat is in the box. The cat likes the box. The box is over the
cat."
Bag of words (stripped punctuation):
"The": 3,
"box": 3
"cat": 3,
"the": 3
"is": 2
"in": 1,
"likes": 1,
"over": 1


Bag-of-words in Python
In [1]: from nltk.tokenize import word_tokenize
In [2]: from collections import Counter
In [3]: Counter(word_tokenize(
"""The cat is in the box. The cat likes the box.
The box is over the cat."""))
Out[3]:
Counter({'.': 3,
'The': 3,
'box': 3,
'cat': 3,
'in': 1,
...
'the': 3})
In [4]: counter.most_common(2)
Out[4]: [('The'
, 3), ('box'
, 3)]


_______________________________________________________________________________________________________________

Building a Counter with bag-of-words
In this exercise, you'll build your first (in this course) bag-of-words counter using a Wikipedia article, which has been pre-loaded as article. Try doing the bag-of-words without looking at the full article text, and guessing what the topic is! If you'd like to peek at the title at the end, we've included it as article_title. Note that this article text has had very little preprocessing from the raw Wikipedia database entry.

word_tokenize has been imported for you.

Instructions
100 XP
Import Counter from collections.
Use word_tokenize() to split the article into tokens.
Use a list comprehension with t as the iterator variable to convert all the tokens into lowercase. The .lower() method converts text into lowercase.
Create a bag-of-words counter called bow_simple by using Counter() with lower_tokens as the argument.
Use the .most_common() method of bow_simple to print the 10 most common tokens.


# Import Counter
from collections import Counter

# Tokenize the article: tokens
tokens = word_tokenize(article)

# Convert the tokens into lowercase: lower_tokens
lower_tokens = [t.lower() for t in tokens]

# Create a Counter with the lowercase tokens: bow_simple
bow_simple = Counter(lower_tokens)

# Print the 10 most common tokens
print(bow_simple.most_common(10))


<script.py> output:
    [(',', 151), ('the', 150), ('.', 89), ('of', 81), ("''", 68), ('to', 63), ('a', 60), ('in', 44), ('and', 41), ('(', 40)]
    
     
___________________________________________________________________________________________________________________________

Simple text
preprocessing


Why preprocess?
Helps make for better input data
When performing machine learning or other statistical methods
Examples:
Tokenization to create a bag of words
Lowercasing words
Lemmatization/Stemming
Shorten words to their root stems
Removing stop words, punctuation, or unwanted tokens
Good to experiment with different approaches


Preprocessing example
Input text: Cats, dogs and birds are common pets. So are fish.
Output tokens: cat, dog, bird, common, pet, fish



Text preprocessing with Python
In [1]: from ntlk.corpus import stopwords
In [2]: text = """The cat is in the box. The cat likes the box.
The box is over the cat."""
In [3]: tokens = [w for w in word_tokenize(text.lower())
if w.isalpha()]
In [4]: no_stops = [t for t in tokens
if t not in stopwords.words('english')]
In [5]: Counter(no_stops).most_common(2)
Out[5]: [('cat'
, 3), ('box'
, 3)]

_________________________________________________________________________________________________________________________


Text preprocessing practice
Now, it's your turn to apply the techniques you've learned to help clean up text for better NLP results. You'll need to remove stop words and non-alphabetic characters, lemmatize, and perform a new bag-of-words on your cleaned text.

You start with the same tokens you created in the last exercise: lower_tokens. You also have the Counter class imported.

Instructions
100 XP
Import the WordNetLemmatizer class from nltk.stem.
Create a list called alpha_only that iterates through lower_tokens and retains only alphabetical characters. You can use the .isalpha() method to check for this.
Create another list called no_stops in which you remove all stop words, which are held in a list called english_stops.
Initialize a WordNetLemmatizer object called wordnet_lemmatizer and use its .lemmatize() method on the tokens in no_stops to create a new list called lemmatized.
Finally, create a new Counter called bow with the lemmatized words and show the 10 most common tokens.


# Import WordNetLemmatizer
from nltk.stem import WordNetLemmatizer

# Retain alphabetic words: alpha_only
alpha_only = [t for t in lower_tokens if t.isalpha()]

# Remove all stop words: no_stops
no_stops = [t for t in alpha_only if t not in english_stops]

# Instantiate the WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()

# Lemmatize all tokens into a new list: lemmatized
lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]

# Create the bag-of-words: bow
bow = Counter(lemmatized)

# Print the 10 most common tokens
print(bow.most_common(10))

<script.py> output:
    [('debugging', 40), ('system', 25), ('software', 16), ('bug', 16), ('problem', 15), ('tool', 15), ('computer', 14), ('process', 13), ('term', 13), ('used', 12)]

__________________________________________________________________________________________________________

Introduction to gensim
What is gensim?
Popular open-source NLP library
Uses top academic models to perform complex tasks
Building document or word vectors
Performing topic identification and document comparison


What is a word vector?



Creating a gensim dictionary
In [1]: from gensim.corpora.dictionary import Dictionary
In [2]: from nltk.tokenize import word_tokenize
In [3]: my_documents = ['The movie was about a spaceship and aliens.'
,
...: 'I really liked the movie!'
,
...: 'Awesome action scenes, but boring characters.'
,
...: 'The movie was awful! I hate alien films.'
,
...: 'Space is cool! I liked the movie.'
,
...: 'More space films, please!'
,]
In [4]: tokenized_docs = [word_tokenize(doc.lower())
...: for doc in my_documents]
In [5]: dictionary = Dictionary(tokenized_docs)
In [6]: dictionary.token2id
Out[6]:
{'!': 11,
'
,
': 17,
'.': 7,
'a': 2,
'about': 4,
...
}



Creating a gensim corpus
In [7]: corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]
In [8]: corpus
Out[8]:
[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)],
[(0, 1), (1, 1), (9, 1), (10, 1), (11, 1), (12, 1)],
...
]

gensim models can be easily saved, updated, and reused
Our dictionary can also be updated
This more advanced and feature rich bag-of-words can be used in
future exercises

______________________________________________________________________________________________________________________________

Creating and querying a corpus with gensim
It's time to apply the methods you learned in the previous video to create your first gensim dictionary and corpus!

You'll use these data structures to investigate word trends and potential interesting topics in your document set. To get started, we have imported a few additional messy articles from Wikipedia, which were preprocessed by lowercasing all words, tokenizing them, and removing stop words and punctuation. These were then stored in a list of document tokens called articles. You'll need to do some light preprocessing and then generate the gensim dictionary and corpus.

Instructions
100 XP
Instructions
100 XP
Import Dictionary from gensim.corpora.dictionary.
Initialize a gensim Dictionary with the tokens in articles.
Obtain the id for "computer" from dictionary. To do this, use its .token2id method which returns ids from text, and then chain .get() which returns tokens from ids. Pass in "computer" as an argument to .get().
Use a list comprehension in which you iterate over articles to create a gensim MmCorpus from dictionary.
In the output expression, use the .doc2bow() method on dictionary with article as the argument.
Print the first 10 word ids with their frequency counts from the fifth document. This has been done for you, so hit 'Submit Answer' to see the results!

# Import Dictionary
from gensim.corpora.dictionary import Dictionary

# Create a Dictionary from the articles: dictionary
dictionary = Dictionary(articles)

# Select the id for "computer": computer_id
computer_id = dictionary.token2id.get("computer")

# Use computer_id with the dictionary to print the word
print(dictionary.get(computer_id))

# Create a MmCorpus: corpus
corpus = [dictionary.doc2bow(article) for article in articles]

# Print the first 10 word ids with their frequency counts from the fifth document
print(corpus[4][:10])

<script.py> output:
    computer
    [(0, 88), (23, 11), (24, 2), (39, 1), (41, 2), (55, 22), (56, 1), (57, 1), (58, 1), (59, 3)]
________________________________________________________________________________________________
Gensim bag-of-words
Now, you'll use your new gensim corpus and dictionary to see the most common terms per document and across all documents. You can use your dictionary to look up the terms. Take a guess at what the topics are and feel free to explore more documents in the IPython Shell!

You have access to the dictionary and corpus objects you created in the previous exercise, as well as the Python defaultdict and itertools to help with the creation of intermediate data structures for analysis.

The fifth document from corpus is stored in the variable doc, which has been sorted in descending order.

Instructions
100 XP
Instructions
100 XP
Print the top five words of bow_doc using each word_id with the dictionary alongside word_count. The word_id can be accessed using the .get() method of dictionary.
Create a defaultdict called total_word_count in which the keys are all the token ids (word_id) and the values are the sum of their occurrence across all documents (word_count). Remember to specify int when creating the defaultdict, and inside the for loop, increment each word_id of total_word_count by word_count.
Create a sorted list from the defaultdict, using words across the entire corpus. To achieve this, use the .items() method on total_word_count inside sorted().
Similar to how you printed the top five words of bow_doc earlier, print the top five words of sorted_word_count as well as the number of occurrences of each word across all the documents.

# Save the fifth document: doc
doc = corpus[4]

# Sort the doc for frequency: bow_doc
bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)

# Print the top 5 words of the document alongside the count
for word_id, word_count in bow_doc[:5]:
    print(dictionary.get(word_id), word_count)
    
# Create the defaultdict: total_word_count
total_word_count = defaultdict(int)
for word_id, word_count in itertools.chain.from_iterable(corpus):
    total_word_count[word_id] += word_count

# Create a sorted list from the defaultdict: sorted_word_count 
sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) 

# Print the top 5 words across all documents alongside the count
for word_id, word_count in sorted_word_count[:5]:
    print(dictionary.get(word_id), word_count)



________________________________________________________________________________________________

Tf-idf with gensim
What is tf-idf?
Term frequency - inverse document frequency
Allows you to determine the most important words in each document
Each corpus may have shared words beyond just stopwords
These words should be down-weighted in importance
Example from astronomy: "Sky"
Ensures most common words don't show up as key words
Keeps document specific frequent words weighted high





Tf-idf formula
w = tf ‚àó log( )
w = tf-idf weight for token i in document j
tf = number of occurences of token i in document j
df = number of documents that contain token i
N = total number of documents



Tf-idf with gensim
In [10]: from gensim.models.tfidfmodel import TfidfModel
In [11]: tfidf = TfidfModel(corpus)
In [12]: tfidf[corpus[1]]
Out[12]:
[(0, 0.1746298276735174),
(1, 0.1746298276735174),
(9, 0.29853166221463673),
(10, 0.7716931521027908),
...
]



===============================================================================================================================
3
Named-entity recognition
0%
This chapter will introduce a slightly more advanced topic - Named-entity recognition. You'll learn how to identify the who, what and where of your texts using pre-trained models on English and non-English text. You'll also learn how to use some new libraries - polyglot and spaCy - to add to your NLP toolbox.






===============================================================================================================================


4
Building a "fake news" classifier
0%
Here, you'll apply the basics of what you've learned along with some supervised machine learning to build a "fake news" detector. You'll begin by learning the basics of supervised machine learning, and then move forward by choosing a few important features and testing ideas to identify and classify "fake news" articles.
